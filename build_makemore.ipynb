{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9736a0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_87996\\3675297486.py:1: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  places = open('data\\places.txt').read().splitlines()\n"
     ]
    }
   ],
   "source": [
    "places = open('data\\places.txt').read().splitlines()\n",
    "places = [p.lower() for p in places]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee9a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c3e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)  # Total number of words in the dataset\n",
    "\n",
    "# Bigram Language Model : Work only on two characters at a time\n",
    "# predict the next character given a previous one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69145dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {}\n",
    "for w in words:\n",
    "    chs=['<S>'] + list(w) + ['<E>']           \n",
    "    for ch1,ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1,ch2)\n",
    "        b[bigram] = b.get(bigram, 0) + 1\n",
    "        # print(ch1,ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc5625",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(b.items(),key = lambda kv : -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c1810e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0101c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data= places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72d02b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(input_data)))) # first we create a list of unique characters\n",
    "s_to_i = { s : i+1 for i,s in enumerate(chars)} # create a mapping from character to index\n",
    "s_to_i['#'] = 0\n",
    "i_to_s= {i:s for s,i in s_to_i.items()} # create a mapping from index to character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40f31fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_distinct_chars = len(s_to_i)  # number of distinct characters\n",
    "N = torch.zeros((no_distinct_chars, no_distinct_chars),dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "415d6997",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for w in input_data:\n",
    "    chs=['#'] + list(w) + ['#']           \n",
    "    for ch1,ch2 in zip(chs, chs[1:]):\n",
    "        ix1= s_to_i[ch1]\n",
    "        ix2= s_to_i[ch2]\n",
    "        # print(i_to_s[ix1], i_to_s[ix2])\n",
    "        N[ix1,ix2]+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833bf937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us visualize the bigram counts\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        ch_str = i_to_s[i] + i_to_s[j]\n",
    "        plt.text(j, i, ch_str, ha='center', va='bottom', color='grey')\n",
    "        plt.text(j, i, N[i,j].item(), ha='center', va='top', color='grey')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22aaa65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float()  # convert to float for probability calculations\n",
    "P /= P.sum(1, keepdim=True)  # normalize to make it a probability distribution\n",
    "#broadcasting is allowed here (for sum) as \n",
    "# 27, 27\n",
    "# 27, 1\n",
    "\n",
    "#what is we have keepdim=False? Then P.sum(1) will return a 1D tensor of 1 by 27\n",
    "# 27, 27  _______>    27, 27 __________> it broadcastes this one onto the 27 columns, # so columns are normalized instead of the rows\n",
    "#     27               1, 27              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9288baff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fqza#\n",
      "qczhatu#\n",
      "losu#\n",
      "fgiy#\n",
      "adeleshoneeonu#\n",
      "ton#\n",
      "zbumoren#\n",
      "mlttodho#\n",
      "t#\n",
      "caganotonbolvwdradwne#\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(23651)\n",
    "for i in range(10):\n",
    "    index=0\n",
    "    chr_str=\"\"\n",
    "    while True:\n",
    "        p_row = P[index]\n",
    "        # p_row= N[index].float()  # probabilities of the next character given the previous one\n",
    "        # p_row /= p_row.sum()  # normalize to make it a probability distribution\n",
    "        \n",
    "        #Sample from the distribution\n",
    "        index = torch.multinomial(p_row, num_samples=1,replacement=True, generator=g).item()  # sample one index from the distribution\n",
    "\n",
    "        curr_chr= i_to_s[index]  # get the character corresponding to the index\n",
    "        chr_str += curr_chr  # append the character to the string\n",
    "        if index == 0:  # if the index is 0, it means we have reached the end of the string\n",
    "            break\n",
    "    print(chr_str)  # print the generated string\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69f2311c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#s -> 0.0627 (-2.7691)\n",
      "se -> 0.0609 (-2.7990)\n",
      "eh -> 0.0133 (-4.3175)\n",
      "ha -> 0.2151 (-1.5369)\n",
      "aj -> 0.0060 (-5.1090)\n",
      "j# -> 0.0222 (-3.8067)\n",
      "Log likelihood of the model: -20.338119506835938\n",
      "Negative log likelihood: 3.3896865844726562\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the quality of the model\n",
    " \n",
    "log_likelihood = 0.0\n",
    "n=0\n",
    "for w in [\"sehaj\"]:\n",
    "    chs = ['#'] + list(w) + ['#']\n",
    "    for ch1, ch2 in zip(chs,chs[1:]):\n",
    "        ix1= s_to_i[ch1]\n",
    "        ix2= s_to_i[ch2]\n",
    "        prob = P[ix1, ix2]  # get the probability of the next character given the previous one\n",
    "        log_prob = torch.log(prob)  # take the log of the probability\n",
    "        log_likelihood += log_prob  # accumulate the log probability\n",
    "        n+=1\n",
    "        print(f'{ch1}{ch2} -> {prob:.4f} ({log_prob:.4f})')  # print the character pair and its probability\n",
    "print(f'Log likelihood of the model: {log_likelihood.item()}')  # print the log likelihood of the model\n",
    "nll=-log_likelihood/n  # negative log likelihood\n",
    "print(f'Negative log likelihood: {nll.item()}')  # print the negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e19e4456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1154, 0.1231, 0.0654, 0.0385, 0.0038, 0.0192, 0.0346, 0.0308,\n",
       "        0.0154, 0.0231, 0.0423, 0.0385, 0.1038, 0.0385, 0.0115, 0.0615, 0.0077,\n",
       "        0.0231, 0.0654, 0.0654, 0.0038, 0.0346, 0.0154, 0.0000, 0.0077, 0.0115])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAMPLING FROM THE MODEL\n",
    "\n",
    "# Since N saves the counts of the bigrams, we first need to convert these counts to probabilities.\n",
    "\n",
    "p = N[0].float() # probabilities of the first character given the start token\n",
    "p /= p.sum() # normalize to make it a probability distribution\n",
    "p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9256fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will  now sample from p using multinomial distribution\n",
    "g = torch.Generator().manual_seed(23651)\n",
    "# p = torch.rand(3, generator=g)\n",
    "# p = p/ p.sum()  # normalize to make it a probability distribution\n",
    "# print(p)\n",
    "ix = torch.multinomial(p, num_samples=1,replacement=True, generator=g).item()  # sample one index from the distribution\n",
    "i_to_s[ix]  # convert the index to character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5deeb88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfa0f32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:  8\n"
     ]
    }
   ],
   "source": [
    "# NEURAL NETWORK APPROACH\n",
    "# We will use a neural network to predict the next character given the previous one.\n",
    "\n",
    "#First let's create the training data of the bigrams (x,y):\n",
    "x_train, y_train = [], []\n",
    "\n",
    "for w in input_data[:1]:\n",
    "    chs=['#'] + list(w) + ['#']           \n",
    "    for ch1,ch2 in zip(chs, chs[1:]):\n",
    "        ix1= s_to_i[ch1] \n",
    "        ix2= s_to_i[ch2]\n",
    "        x_train.append(ix1)\n",
    "        y_train.append(ix2)\n",
    "\n",
    "x_train = torch.tensor(x_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "num = x_train.nelement()\n",
    "print('Number of examples: ', num)\n",
    "\n",
    "\n",
    "#INITIALIZING THE NETWORK\n",
    "#randomly initializing 27 neurons' weights. Each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(12345)\n",
    "W= torch.randn((no_distinct_chars,no_distinct_chars), generator=g, requires_grad=True) #27x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6f05a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3.943207025527954\n",
      "Epoch 2/100, Loss: 2.7272212505340576\n",
      "Epoch 3/100, Loss: 1.7540009021759033\n",
      "Epoch 4/100, Loss: 1.0856691598892212\n",
      "Epoch 5/100, Loss: 0.7010617256164551\n",
      "Epoch 6/100, Loss: 0.5121611952781677\n",
      "Epoch 7/100, Loss: 0.4181542992591858\n",
      "Epoch 8/100, Loss: 0.3650933504104614\n",
      "Epoch 9/100, Loss: 0.3315322995185852\n",
      "Epoch 10/100, Loss: 0.3085062503814697\n",
      "Epoch 11/100, Loss: 0.29176610708236694\n",
      "Epoch 12/100, Loss: 0.2790646553039551\n",
      "Epoch 13/100, Loss: 0.26910707354545593\n",
      "Epoch 14/100, Loss: 0.2610972225666046\n",
      "Epoch 15/100, Loss: 0.25451838970184326\n",
      "Epoch 16/100, Loss: 0.24902157485485077\n",
      "Epoch 17/100, Loss: 0.24436213076114655\n",
      "Epoch 18/100, Loss: 0.24036401510238647\n",
      "Epoch 19/100, Loss: 0.23689690232276917\n",
      "Epoch 20/100, Loss: 0.23386260867118835\n",
      "Epoch 21/100, Loss: 0.23118562996387482\n",
      "Epoch 22/100, Loss: 0.22880707681179047\n",
      "Epoch 23/100, Loss: 0.22668024897575378\n",
      "Epoch 24/100, Loss: 0.2247675210237503\n",
      "Epoch 25/100, Loss: 0.22303856909275055\n",
      "Epoch 26/100, Loss: 0.22146844863891602\n",
      "Epoch 27/100, Loss: 0.22003653645515442\n",
      "Epoch 28/100, Loss: 0.21872542798519135\n",
      "Epoch 29/100, Loss: 0.21752089262008667\n",
      "Epoch 30/100, Loss: 0.21641044318675995\n",
      "Epoch 31/100, Loss: 0.21538369357585907\n",
      "Epoch 32/100, Loss: 0.2144317775964737\n",
      "Epoch 33/100, Loss: 0.21354669332504272\n",
      "Epoch 34/100, Loss: 0.21272198855876923\n",
      "Epoch 35/100, Loss: 0.2119516283273697\n",
      "Epoch 36/100, Loss: 0.21123050153255463\n",
      "Epoch 37/100, Loss: 0.21055418252944946\n",
      "Epoch 38/100, Loss: 0.20991863310337067\n",
      "Epoch 39/100, Loss: 0.20932042598724365\n",
      "Epoch 40/100, Loss: 0.20875637233257294\n",
      "Epoch 41/100, Loss: 0.2082236409187317\n",
      "Epoch 42/100, Loss: 0.20771978795528412\n",
      "Epoch 43/100, Loss: 0.20724251866340637\n",
      "Epoch 44/100, Loss: 0.20678992569446564\n",
      "Epoch 45/100, Loss: 0.2063600867986679\n",
      "Epoch 46/100, Loss: 0.20595139265060425\n",
      "Epoch 47/100, Loss: 0.20556238293647766\n",
      "Epoch 48/100, Loss: 0.20519161224365234\n",
      "Epoch 49/100, Loss: 0.20483793318271637\n",
      "Epoch 50/100, Loss: 0.20450016856193542\n",
      "Epoch 51/100, Loss: 0.20417733490467072\n",
      "Epoch 52/100, Loss: 0.20386840403079987\n",
      "Epoch 53/100, Loss: 0.2035725712776184\n",
      "Epoch 54/100, Loss: 0.20328912138938904\n",
      "Epoch 55/100, Loss: 0.20301717519760132\n",
      "Epoch 56/100, Loss: 0.20275607705116272\n",
      "Epoch 57/100, Loss: 0.20250527560710907\n",
      "Epoch 58/100, Loss: 0.20226411521434784\n",
      "Epoch 59/100, Loss: 0.20203205943107605\n",
      "Epoch 60/100, Loss: 0.20180869102478027\n",
      "Epoch 61/100, Loss: 0.20159347355365753\n",
      "Epoch 62/100, Loss: 0.20138601958751678\n",
      "Epoch 63/100, Loss: 0.20118595659732819\n",
      "Epoch 64/100, Loss: 0.20099283754825592\n",
      "Epoch 65/100, Loss: 0.20080623030662537\n",
      "Epoch 66/100, Loss: 0.20062601566314697\n",
      "Epoch 67/100, Loss: 0.2004517912864685\n",
      "Epoch 68/100, Loss: 0.2002832442522049\n",
      "Epoch 69/100, Loss: 0.20012010633945465\n",
      "Epoch 70/100, Loss: 0.19996219873428345\n",
      "Epoch 71/100, Loss: 0.19980907440185547\n",
      "Epoch 72/100, Loss: 0.19966088235378265\n",
      "Epoch 73/100, Loss: 0.19951704144477844\n",
      "Epoch 74/100, Loss: 0.19937758147716522\n",
      "Epoch 75/100, Loss: 0.19924218952655792\n",
      "Epoch 76/100, Loss: 0.19911082088947296\n",
      "Epoch 77/100, Loss: 0.19898319244384766\n",
      "Epoch 78/100, Loss: 0.19885914027690887\n",
      "Epoch 79/100, Loss: 0.19873857498168945\n",
      "Epoch 80/100, Loss: 0.19862139225006104\n",
      "Epoch 81/100, Loss: 0.19850735366344452\n",
      "Epoch 82/100, Loss: 0.19839641451835632\n",
      "Epoch 83/100, Loss: 0.19828838109970093\n",
      "Epoch 84/100, Loss: 0.19818325340747833\n",
      "Epoch 85/100, Loss: 0.19808082282543182\n",
      "Epoch 86/100, Loss: 0.19798095524311066\n",
      "Epoch 87/100, Loss: 0.1978837102651596\n",
      "Epoch 88/100, Loss: 0.19778886437416077\n",
      "Epoch 89/100, Loss: 0.19769632816314697\n",
      "Epoch 90/100, Loss: 0.197606161236763\n",
      "Epoch 91/100, Loss: 0.19751803576946259\n",
      "Epoch 92/100, Loss: 0.19743210077285767\n",
      "Epoch 93/100, Loss: 0.19734816253185272\n",
      "Epoch 94/100, Loss: 0.19726616144180298\n",
      "Epoch 95/100, Loss: 0.19718609750270844\n",
      "Epoch 96/100, Loss: 0.19710786640644073\n",
      "Epoch 97/100, Loss: 0.1970313936471939\n",
      "Epoch 98/100, Loss: 0.19695666432380676\n",
      "Epoch 99/100, Loss: 0.19688349962234497\n",
      "Epoch 100/100, Loss: 0.1968119740486145\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GRADIENT DESCENT\n",
    "epochs = 100\n",
    "lr= 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    #forward pass:\n",
    "    x_enc= F.one_hot(x_train, num_classes=no_distinct_chars).float() #input to the network : one-hot encoding\n",
    "    logits = x_enc @ W # log-counts\n",
    "    counts = logits.exp() #Equivalent N\n",
    "    prob = counts / counts.sum(1, keepdim=True)  # normalize to make it a probability distribution (softmax)\n",
    "    loss = -prob[torch.arange(x_enc.shape[0]), y_train].log().mean() # negative log likelihood loss\n",
    "    loss += 0.01*(W**2).mean() #Regularization Loss\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    # backward pass\n",
    "    W.grad= None  # reset the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # update the weights\n",
    "    W.data += -lr * W.grad  # gradient descent step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a429d444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 27])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bb8d29a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(x_enc, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(x_enc, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d939c28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      " bigram example 1: #a (indices: 0, 1)\n",
      "input to the neural net:0\n",
      "output probabilities from the neural net: tensor([7.0330e-05, 9.9138e-01, 1.6404e-05, 3.6166e-04, 3.3996e-04, 2.5487e-04,\n",
      "        3.0555e-04, 8.4465e-05, 9.6687e-04, 8.5261e-04, 4.5826e-04, 1.0724e-04,\n",
      "        2.1680e-04, 4.1186e-04, 5.0365e-04, 2.9817e-04, 5.1224e-04, 9.6389e-05,\n",
      "        2.5768e-04, 6.2985e-04, 1.0769e-04, 1.6356e-04, 3.5822e-04, 4.6327e-05,\n",
      "        6.1584e-04, 9.6876e-05, 4.8372e-04], grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the next character : 0.9913830161094666\n",
      "log likelihood :  -0.00865432433784008\n",
      "negative log likelihood :  0.00865432433784008\n",
      "---------------\n",
      " bigram example 2: ab (indices: 1, 2)\n",
      "input to the neural net:1\n",
      "output probabilities from the neural net: tensor([4.9316e-04, 3.9587e-04, 4.9588e-01, 3.7766e-04, 2.7098e-04, 2.0364e-04,\n",
      "        2.0550e-04, 2.7974e-04, 3.0508e-04, 2.3124e-04, 3.6907e-04, 2.6861e-04,\n",
      "        6.3462e-04, 3.8325e-04, 4.9588e-01, 2.2736e-04, 5.5087e-04, 5.7939e-05,\n",
      "        3.9161e-04, 4.7933e-04, 1.1909e-04, 1.5577e-04, 3.6845e-04, 3.6408e-04,\n",
      "        4.4788e-04, 5.0739e-04, 1.5956e-04], grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 2\n",
      "probability assigned by the net to the next character : 0.49587613344192505\n",
      "log likelihood :  -0.7014291286468506\n",
      "negative log likelihood :  0.7014291286468506\n",
      "---------------\n",
      " bigram example 3: bi (indices: 2, 9)\n",
      "input to the neural net:2\n",
      "output probabilities from the neural net: tensor([4.3465e-04, 4.2442e-04, 3.6830e-04, 1.9327e-04, 5.9538e-05, 3.6766e-04,\n",
      "        1.4545e-04, 2.6026e-04, 1.8818e-04, 9.9122e-01, 1.9549e-04, 1.3436e-04,\n",
      "        2.2532e-04, 4.6917e-04, 6.2339e-04, 7.5265e-04, 8.0173e-05, 1.4695e-04,\n",
      "        1.4360e-04, 1.9154e-04, 4.8719e-04, 2.4635e-04, 7.1602e-04, 5.3724e-05,\n",
      "        4.3058e-04, 8.7291e-04, 5.6882e-04], grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 9\n",
      "probability assigned by the net to the next character : 0.9912198781967163\n",
      "log likelihood :  -0.008818894624710083\n",
      "negative log likelihood :  0.008818894624710083\n",
      "---------------\n",
      " bigram example 4: id (indices: 9, 4)\n",
      "input to the neural net:9\n",
      "output probabilities from the neural net: tensor([2.6474e-04, 1.9566e-04, 3.9290e-04, 2.4748e-04, 9.9115e-01, 3.0045e-04,\n",
      "        5.0510e-04, 3.2071e-04, 3.7155e-04, 5.5434e-04, 2.2066e-04, 1.6513e-04,\n",
      "        4.2357e-04, 2.5710e-04, 3.8548e-04, 1.4239e-04, 1.5663e-04, 6.1934e-04,\n",
      "        2.9041e-04, 6.2572e-04, 5.9355e-04, 4.1049e-05, 7.5607e-04, 2.5431e-04,\n",
      "        2.3346e-04, 4.7409e-05, 4.8168e-04], grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 4\n",
      "probability assigned by the net to the next character : 0.9911531805992126\n",
      "log likelihood :  -0.0088861845433712\n",
      "negative log likelihood :  0.0088861845433712\n",
      "---------------\n",
      " bigram example 5: dj (indices: 4, 10)\n",
      "input to the neural net:4\n",
      "output probabilities from the neural net: tensor([2.8625e-04, 1.7319e-04, 4.8027e-04, 2.9235e-04, 2.5417e-04, 6.7184e-04,\n",
      "        3.0520e-04, 3.8057e-04, 1.4611e-04, 4.7403e-04, 9.9136e-01, 3.1049e-04,\n",
      "        5.2285e-04, 5.9188e-05, 8.4856e-05, 1.8211e-04, 2.6382e-04, 2.2498e-04,\n",
      "        3.5997e-04, 6.6248e-04, 3.7520e-04, 1.2255e-04, 2.5331e-04, 1.4290e-04,\n",
      "        7.8839e-05, 1.0296e-03, 5.0545e-04], grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 10\n",
      "probability assigned by the net to the next character : 0.9913574457168579\n",
      "log likelihood :  -0.008680117316544056\n",
      "negative log likelihood :  0.008680117316544056\n",
      "---------------\n",
      " bigram example 6: ja (indices: 10, 1)\n",
      "input to the neural net:10\n",
      "output probabilities from the neural net: tensor([3.8099e-04, 9.9133e-01, 7.6443e-04, 9.2301e-05, 7.7194e-05, 3.3386e-04,\n",
      "        2.0761e-04, 1.3477e-04, 4.2687e-04, 2.2483e-04, 4.5917e-04, 6.2591e-04,\n",
      "        4.6055e-04, 3.2589e-04, 3.1885e-04, 5.1402e-04, 1.8650e-04, 3.0870e-04,\n",
      "        2.6992e-04, 8.4089e-05, 2.2648e-04, 4.7527e-04, 4.1796e-04, 4.9522e-04,\n",
      "        1.0715e-04, 2.6163e-04, 4.8742e-04], grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the next character : 0.9913324117660522\n",
      "log likelihood :  -0.008705370128154755\n",
      "negative log likelihood :  0.008705370128154755\n",
      "---------------\n",
      " bigram example 7: an (indices: 1, 14)\n",
      "input to the neural net:1\n",
      "output probabilities from the neural net: tensor([4.9316e-04, 3.9587e-04, 4.9588e-01, 3.7766e-04, 2.7098e-04, 2.0364e-04,\n",
      "        2.0550e-04, 2.7974e-04, 3.0508e-04, 2.3124e-04, 3.6907e-04, 2.6861e-04,\n",
      "        6.3462e-04, 3.8325e-04, 4.9588e-01, 2.2736e-04, 5.5087e-04, 5.7939e-05,\n",
      "        3.9161e-04, 4.7933e-04, 1.1909e-04, 1.5577e-04, 3.6845e-04, 3.6408e-04,\n",
      "        4.4788e-04, 5.0739e-04, 1.5956e-04], grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 14\n",
      "probability assigned by the net to the next character : 0.49587613344192505\n",
      "log likelihood :  -0.7014291286468506\n",
      "negative log likelihood :  0.7014291286468506\n",
      "---------------\n",
      " bigram example 8: n# (indices: 14, 0)\n",
      "input to the neural net:14\n",
      "output probabilities from the neural net: tensor([9.9125e-01, 2.1509e-04, 5.7449e-04, 6.0236e-04, 5.2270e-04, 7.6195e-05,\n",
      "        3.5044e-04, 7.1403e-04, 2.9169e-04, 1.3179e-04, 3.0597e-04, 5.8746e-04,\n",
      "        1.1399e-04, 6.4333e-04, 3.3548e-04, 2.1403e-04, 2.0306e-04, 6.5408e-05,\n",
      "        2.1088e-04, 2.5739e-04, 2.3694e-04, 4.7947e-04, 6.9938e-04, 8.8132e-05,\n",
      "        6.9989e-05, 1.3371e-04, 6.2160e-04], grad_fn=<SelectBackward0>)\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the next character : 0.991254985332489\n",
      "log likelihood :  -0.008783476427197456\n",
      "negative log likelihood :  0.008783476427197456\n",
      "======================================\n",
      "avergage negative log likelihood: 0.18192331492900848\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(x_enc.shape[0])\n",
    "for i in range(x_enc.shape[0]):\n",
    "    # i-th bigram:\n",
    "    x = x_train[i].item() #input character index\n",
    "    y = y_train[i].item() #output character index\n",
    "    print(\"---------------\")\n",
    "    print(f' bigram example {i+1}: {i_to_s[x]}{i_to_s[y]} (indices: {x}, {y})')\n",
    "    print(f'input to the neural net:{x}')\n",
    "    print('output probabilities from the neural net:',prob[i])\n",
    "    print('label (actual next character):', y)\n",
    "    p = prob[i,y]\n",
    "    print('probability assigned by the net to the next character :', p.item())\n",
    "    logp= torch.log(p)  # log probability\n",
    "    print(\"log likelihood : \", logp.item())\n",
    "    nll=-logp  # negative log likelihood\n",
    "    print(\"negative log likelihood : \", nll.item())\n",
    "    nlls[i] = nll  # store the negative log likelihood\n",
    "\n",
    "print(\"======================================\")\n",
    "print(\"avergage negative log likelihood:\", nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from the neural net model\n",
    "g = torch.Generator().manual_seed(23651)\n",
    "W = torch.randn((no_distinct_chars, no_distinct_chars), generator=g, requires_grad=True)  # reinitialize W for sampling\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    out =[]\n",
    "    index=0\n",
    "    while True:\n",
    "\n",
    "        x_enc= F.one_hot(torch.tensor([index]), num_classes=no_distinct_chars).float()\n",
    "        logits = x_enc @ W  # log-counts\n",
    "        counts = logits.exp() # Equivalent N\n",
    "        prob = counts / counts.sum(1, keepdim=True)  # normalize to make it a probability distribution (softmax)\n",
    "        \n",
    "        index = torch.multinomial(prob, num_samples=1,replacement=True, generator=g).item()  # sample one index from the distribution\n",
    "        curr_chr= i_to_s[index]  # get the character corresponding to the index\n",
    "        out.append(curr_chr)  # append the character to the string\n",
    "\n",
    "        if index == 0:  # if the index is 0, it means we have reached the end of the string\n",
    "            break\n",
    "    print(''.join(out))  # print the generated string\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
